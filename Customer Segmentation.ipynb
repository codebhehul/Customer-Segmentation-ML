{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Business Problem\n",
    "## 1.1 Problem Context\n",
    "Our client is an online retailer based in the UK. They sell all-occasion gifts, and many of their customers are wholesalers.\n",
    "* Most of their customers are from the UK, but they have a small percent of customers from other countries.\n",
    "* They want to create groups of these international customers based on their previous purchase patterns.\n",
    "* Their goal is to provide more tailored services and improve the way they market to these international customers.\n",
    "\n",
    "## 1.2 Problems with current approach\n",
    "Currently, the retailer simply groups their international customers by country. As you'll see in the project, this is quite inefficient because:\n",
    "* There's a large number of countries (which kind of defeats the purpose of creating groups).\n",
    "* Some countries have very few customers.\n",
    "* This approach treats large and small customers the same, regardless of their purchase patterns.\n",
    "\n",
    "## 1.3 Problem Statement\n",
    "The retailer has hired us to help them create customer clusters, a.k.a **\"customer segments\"** through a data-driven approach.\n",
    "* They've provided us a dataset of past purchase data at the transaction level.\n",
    "* Our task is to build a clustering model using that dataset.\n",
    "* Our clustering model should factor in both aggregate sales patterns and specific items purchased.\n",
    "\n",
    "## 1.4 Business Objectives and constraints\n",
    "\n",
    "# 2. Machine Learning Problem\n",
    "## 2.1 Data Overview\n",
    "For this project:\n",
    "* The dataset has 35116 observations for previous international transactions.\n",
    "* The observations span 37 different countries.\n",
    "* **Note:** There is no target variable.\n",
    "\n",
    "We have the following features:\n",
    "\n",
    "Invoice information\n",
    "* 'InvoiceNo' – Unique ID for invoice\n",
    "* 'InvoiceDate' – Invoice date\n",
    "\n",
    "Item information\n",
    "* 'StockCode' – Unique ID for item\n",
    "* 'Description' – Text description for item\n",
    "* 'Quantity' – Units per pack for item\n",
    "* 'UnitPrice' – Price per unit in GBP\n",
    "\n",
    "Customer information\n",
    "* 'CustomerID' – Unique ID for customer\n",
    "* 'Country' – Country of customer\n",
    "\n",
    "## 2.2 Mapping Buisness problem to ML problem\n",
    "### 2.2.1 Type of ML Problem\n",
    "It is an unsupervised learning task, where given the features about each transaction, we need to segment the customers based on their buying patterns.\n",
    "* It is importnat to note that the given data is transaction-level while the clusters (or segmenst) we need to create are customer-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis\n",
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "# display plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# StandardScaler from Scikit-Learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# PCA from Scikit-Learn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scikit-Learn's KMeans algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Adjusted Rand index\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load international online transactions data from CSV\n",
    "df = pd.read_csv('Files/int_online_tx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe dimension\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 10 rows of data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some questions to consider:\n",
    "* In the first 10 observations, how many different customers are there?\n",
    "* How many different invoices are there?\n",
    "* Given answers to the first two questions, how many unique purchases are shown?\n",
    "* Technically, aren't these observations actually line-items within each transaction? \n",
    "* Do you expect the customer-level dataset to be much smaller?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the distribution of transactions by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make figsize\n",
    "plt.figure(figsize=(9,10))\n",
    "\n",
    "# Bar plot by country\n",
    "sns.countplot(y='Country', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many **sparse classes**. Countries like Lithuania, Brazil, and even the USA have a tiny number of transactions.\n",
    "\n",
    "**Note:** This is at the transaction/line-item level. The number of customers for each country is even smaller because each customer has multiple transactions! Therefore, it's plain to see that clustering by country is not very efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transaction-level Cleaning\n",
    "Before we aggregate to the customer level, we need to tidy up a few things at the transaction level.\n",
    "* Technically, this is the **\"line-item\" level** because one invoice (a.k.a. transaction) spans multiple rows. However, we'll just refer to it as the \"transaction level\" for simplicity.\n",
    "* Also the terms **\"aggegrating up\"** and **\"rolling up\"** are used interchangeability.\n",
    "\n",
    "First, we need to check for any missing data before rolling up. If we don't, we might get confusing results later because rolling up to a higher level can sometimes hide the fact that data was missing at the lower level.\n",
    "\n",
    "Display the number of missing observations for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data by feature\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'CustomerID' has missing observations\n",
    "* Should we **label them as missing** (as for categorical features) or should we **flag and fill** them (as for numeric features)?\n",
    "* We should do neither. Instead, we simply need to drop transactions with missing CustomerID.\n",
    "* Think back to the project scope: We are trying to cluster customers in order to provide more tailored service!\n",
    "* That means transactions with missing 'CustomerID' are actually pointless to keep.\n",
    "* In other words, they should be considered \"unwanted observations\" instead of \"missing data!\".\n",
    "\n",
    "Drop observations with missing customer ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep transactions with CustomerID's\n",
    "df = df[df.CustomerID.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, just for clarity, convert the CustomerID's from floats into integers. (This is technically not required, but it's good practice to save ID's as either strings or integers so they don't get mixed up with other numeric features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert customer ID's into integers\n",
    "df['CustomerID'] = df.CustomerID.astype(int)\n",
    "\n",
    "# Display first 5 CustomerID's in the transaction dataset\n",
    "df.CustomerID.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one feature we need to create at the transaction level.\n",
    "\n",
    "Look back at the first 10 observations you displayed earlier.\n",
    "* Are there any features that tell you how much money the customer spent on each transaction?\n",
    "* Well, we have 'Quantity' and 'UnitPrice', but those are for individual units, not for the transaction.\n",
    "* We need to multiply them together to get the Sales amount for that transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Sales' interaction feature\n",
    "df['Sales'] = df.Quantity * df.UnitPrice\n",
    "\n",
    "# Display first 5 Sales values in the transaction dataset\n",
    "df.Sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, save your cleaned transaction-level data as **cleaned_transactions.csv.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned transaction-level data\n",
    "df.to_csv('Files/cleaned_transactions.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Customer-level feature engineering\n",
    "Now that we have a cleaned transaction-level dataset, it's time to **roll it up** (aggregate up) to the customer level, which we'll feed into our machine learning algorithms later.\n",
    "\n",
    "We want 1 customer per row, and we want the features to represent information such as:\n",
    "* Number of unique purchases by the customer\n",
    "* Average cart value for the customer\n",
    "* Total sales for the customer\n",
    "* Etc.\n",
    "\n",
    "To do so, we'll use two tools seen already:\n",
    "* groupby() to roll up by customer.\n",
    "* agg() to engineer aggregated features.\n",
    "\n",
    "Aggegrate invoice data by customer. We'll engineer 1 feature:\n",
    "* 'total_transactions' - the total number of unique transactions for each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggegrate invoice data\n",
    "invoice_data = df.groupby('CustomerID').InvoiceNo.agg({ 'total_transactions' : 'nunique' })\n",
    "\n",
    "# Display invoice data for first 5 customers\n",
    "invoice_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate product data\n",
    "product_data = df.groupby('CustomerID').StockCode.agg( { 'total_products' : 'count', \n",
    "                                                     'total_unique_products' : 'nunique' } )\n",
    "\n",
    "# Display product data for first 5 customers\n",
    "product_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, 'total_unique_products' should always be less than or equal to 'total_products'.\n",
    "\n",
    "Finally, aggregate sales data by customer. Engineer 2 features:\n",
    "* 'total_sales' - the total sales for each customer.\n",
    "* 'avg_product_value' - the average value of the products purchased by the customer (not the UnitPrice!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roll up sales data\n",
    "sales_data = df.groupby('CustomerID').Sales.agg( { 'total_sales' : 'sum', \n",
    "                                                  'avg_product_value' : 'mean' } )\n",
    "\n",
    "# Display sales data for first 5 customers\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Intermediary levels\n",
    "You won't always be able to easily roll up to customer-level directly. Sometimes, it will be easier to create intermediary levels first.\n",
    "\n",
    "For example, let's say we wanted to calculate the average cart value for each customer.\n",
    "* 'avg_product_value' isn't the same thing because it doesn't first group products that were purchased within the same \"cart\" (i.e. invoice).\n",
    "\n",
    "Therefore, let's first aggregate cart data at the \"cart-level.\"\n",
    "* We'll group by 'CustomerID' AND by 'InvoiceID'. Remember, we're treating each invoice as a \"cart.\"\n",
    "* Then, we'll calculate 'cart_value' by taking the sum of the Sales column. This is the total sales by invoice (i.e. cart).\n",
    "* Finally, we'll call .reset_index() to turn CustomerID and InvoiceID back into regular columns so we can perform another aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate cart-level data (i.e. invoice-level)\n",
    "cart_data = df.groupby(['CustomerID' , 'InvoiceNo']).Sales.agg( { 'cart_value' : 'sum' })\n",
    "\n",
    "# Display cart data for first 20 CARTS\n",
    "cart_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "cart_data.reset_index(inplace=True)\n",
    "\n",
    "# Display cart data for first 10 CARTS\n",
    "cart_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cart-level cart data, all we need to do is roll up by CustomerID again to get customer-level cart data.\n",
    "\n",
    "Aggregate cart data by customer. Engineer 3 features:\n",
    "* 'avg_cart_value' - average cart value by customer.\n",
    "* 'min_cart_value' - minimum cart value by customer.\n",
    "* 'max_cart_value' - maximum cart value by customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate cart data (at customer-level)\n",
    "agg_cart_data = cart_data.groupby('CustomerID').cart_value.agg( { 'avg_cart_value' : 'mean', \n",
    "                                                                 'min_cart_value' : 'min',\n",
    "                                                                 'max_cart_value' : 'max'})\n",
    "\n",
    "# Display cart data for first 5 CUSTOMERS\n",
    "agg_cart_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Joining various customer level dataframes\n",
    "We have multiple dataframes that each contain customer-level features:\n",
    "* invoice_data\n",
    "* product_data\n",
    "* sales_data\n",
    "* agg_cart_data\n",
    "\n",
    "Let's join the various customer-level datasets together with the .join() function.\n",
    "* Just pick one of the customer-level dataframes and join it to a list of the others.\n",
    "* By default, it will join the dataframes on their index... In this case, it will join by CustomerID, which is exactly what we want.\n",
    "* You can read more about the .join() function in the official documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join together customer-level data\n",
    "customer_df = invoice_data.join([product_data, sales_data, agg_cart_data])\n",
    "\n",
    "# Display customer-level data for first 5 customers\n",
    "customer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save customer_df as our **analytical base table** to use later.\n",
    "\n",
    "**Very Important:** We will not set index=None because we want to keep the CustomerID's as the index (this will be important and we'll see later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analytical base table\n",
    "customer_df.to_csv('Files/analytical_base_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Curse of Dimensionality.\n",
    "Let's import the cleaned dataset (not the analytical base table) that we saved in previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cleaned_transactions.csv\n",
    "df = pd.read_csv('Files/cleaned_transactions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 So what is \"The Curse of Dimensionality?\"\n",
    "\n",
    "\"dimensionality\" refers to the number of features in your dataset. The basic idea is that as the number of features increases, you'll need more and more observations to build any sort of meaningful model, especially for clustering.\n",
    "\n",
    "Because cluster models are based on the \"distance\" between two observations, and distance is calculated by taking the differences between feature values, every observation will seem \"far away\" from each other if the number of features increases.\n",
    "\n",
    "The reference link provides an excellent analogy : https://www.quora.com/What-is-the-curse-of-dimensionality\n",
    "\n",
    "    Let's say you have a straight line 100 yards long and you dropped a penny somewhere on it. It wouldn't be too hard to find. You walk along the line and it takes two minutes.\n",
    "\n",
    "    Now let's say you have a square 100 yards on each side and you dropped a penny somewhere on it. It would be pretty hard, like searching across two football fields stuck together. It could take days.\n",
    "\n",
    "    Now a cube 100 yards across. That's like searching a 30-story building the size of a football stadium. Ugh.\n",
    "\n",
    "    The difficulty of searching through the space gets a lot harder as you have more dimensions.\n",
    "\n",
    "For our practical purposes, it's enough to remember that when you have many features (high dimensionality), it makes clustering especially hard because every observation is \"far away\" from each other.\n",
    "\n",
    "The amount of \"space\" that a data point could potentially exist in becomes larger and larger, and clusters become very hard to form.\n",
    "\n",
    "## 8.2 Item Data\n",
    "So how does The Curse of Dimensionality arise in this problem?\n",
    "\n",
    "Well, in the previous module, we created a customer-level analytical base table with important features such as total sales by customer and average cart value by customer.\n",
    "\n",
    "However, remember, the client would also like to to include information about individual items that were purchased.\n",
    "* For example, if two customers purchased similar items, our model should be more likely to group them into the same cluster.\n",
    "* In other words, we care not just about how much a customer purchases, but also what they purchase.\n",
    "* In every observation, along with the data that we have built, we need to include the information of what are the products that are purchased by each customer, i.e. we need to find some way to represent each unique item.\n",
    "\n",
    "One way is to create a vector of unique values of the StockCode column and if the customer has purchased a particular product, fill it by 1, else fill it by 0.\n",
    "\n",
    "This is like the binary CountVectorizer technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get item_dummies - creates the vector of StockCode\n",
    "item_dummies = pd.get_dummies(df.StockCode)\n",
    "\n",
    "item_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add 'CustomerID' to this new dataframe so that we can roll up (aggregate) by customer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CustomerID to item_dummies\n",
    "item_dummies['CustomerID'] = df.CustomerID\n",
    "\n",
    "# Display first 5 rows of item_dummies\n",
    "item_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, roll up the item dummies data into customer-level item data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create item_data by aggregating at customer level\n",
    "item_data = item_dummies.groupby('CustomerID').sum()\n",
    "\n",
    "# Display first 5 rows of item_data\n",
    "item_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even after rolling up to the customer level, most of the values are still 0. That means that most customers are not buying a huge array of different items, which is to be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's display the total number times each item was purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total times each item was purchased\n",
    "item_data.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most items were purchased less than a handful of times!\n",
    "\n",
    "First of all, we've just created 2574 customer-level item features, which leads to The Curse of Dimensionality.\n",
    "To make matters even worse, most of the values for many of those features are 0!\n",
    "\n",
    "So, we'll introduce a couple of strategies for reducing the number of item features that we actually keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let's save this customer-level item dataframe as 'item_data.csv'. We'll use it again in the next module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save item_data.csv\n",
    "item_data.to_csv('Files/item_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Method 1 - Thresholding\n",
    "One very simple and straightforward way to reduce the dimensionality of this item data is to set a threshold for keeping features.\n",
    "* The rationale is that you might only want to keep **popular items.**\n",
    "* For example, let's say item A was only purchased by 2 customers. Well, the feature for item A will be 0 for almost all observations, which isn't very helpful.\n",
    "* On the other hand, let's say item B was purchased by 100 customers. The feature for item B will allow more meaningful comparisons.\n",
    "\n",
    "To make this concrete, assume we only wish to keep item features for the 20 most popular items.\n",
    "\n",
    "First, we can see which items those are and the number of times they were purchased.\n",
    "* Take the sum by column.\n",
    "* Sort the values.\n",
    "* Look at the last 20 (since they are sorted in ascending order by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display most popular 20 items\n",
    "item_data.sum().sort_values().tail(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, if we take the .index of the above series, we can get just a list of the StockCodes for those 20 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of StockCodes for the 20 most popular items\n",
    "top_20_items = item_data.sum().sort_values().tail(120).index\n",
    "\n",
    "top_20_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can keep only the features for those 20 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only features for top 20 items\n",
    "top_20_item_data = item_data[top_20_items]\n",
    "\n",
    "# Shape of remaining dataframe\n",
    "top_20_item_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_item_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 20 features are much more manageable than the 2574 from earlier, and they are arguably the most important features because they are the most popular items.\n",
    "\n",
    "Finally, save this top 20 items dataframe as 'threshold_item_data.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save threshold_item_data.csv\n",
    "top_20_item_data.to_csv('Files/threshold_item_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 PCA\n",
    "Let's import the full item data that we saved in the previous module (before applying thresholds)\n",
    "\n",
    "This time, we'll also pass in the argument index_col=0 to tell Pandas to treat the first column (CustomerID) as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read item_data.csv\n",
    "item_data = pd.read_csv('Files/item_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display item_data's shape\n",
    "item_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is an Unsupervised Learning task that creates a sequence of new, uncorrelated features that each try to maximize its \"explained variance\" of the original dataset.\n",
    "* It does so by generating linear combinations from your original features.\n",
    "* These new features are meant to replace the original ones.\n",
    "\n",
    "Here's where dimensionality reduction comes into play, and it's brilliantly simple:\n",
    "* You don't need to keep all of the principal components!\n",
    "* You can just keep some number of the \"best\" components, a.k.a. the ones that explain the most variance.\n",
    "* Remember, PCA creates a sequence of principal components and each one tries to capture the most variance after accounting for the ones before it.\n",
    "\n",
    "First, **scale item_data**, which you imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform item_data\n",
    "item_data_scaled = scaler.fit_transform(item_data)\n",
    "\n",
    "# Display first 5 rows of item_data_scaled\n",
    "item_data_scaled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, initialize and fit an instance of the PCA transformation.\n",
    "\n",
    "Keep all of the components for now (just don't pass in any argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit a PCA transformation\n",
    "pca = PCA()\n",
    "pca.fit(item_data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, generate new \"principal component features\" from item_data_scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new features\n",
    "PC_items = pca.transform(item_data_scaled)\n",
    "\n",
    "# Display first 5 rows\n",
    "PC_items[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance\n",
    "It's very helpful to calculate and plot the cumulative explained variance.\n",
    "* This will tell us the total amount of variance we'd capture if we kept up to the n-th component.\n",
    "* First, we'll use np.cumsum() to calculate the cumulative explained variance.\n",
    "* Then, we'll plot it so we can see how many PC features we'd need to keep in order to capture most of the original variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.grid()\n",
    "plt.plot(range(len(cumulative_explained_variance)), cumulative_explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart is saying: To capture about 98% of the variance, we'd need to keep around 300 components.\n",
    "\n",
    "We can confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much variance we'd capture with the first 125 components\n",
    "cumulative_explained_variance[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing 2574 features down to 300 (about 88% fewer features) while capturing almost 80% of the original variance is certainly not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and fit another PCA transformation.\n",
    "* This time, only keep 300 components.\n",
    "* Generate the principal component features from the fitted instance and name the new matrix PC_items.\n",
    "* Then, display the shape of PC_items to confirm it only has 300 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=300)\n",
    "\n",
    "# Fit and transform item_data_scaled\n",
    "PC_items = pca.fit_transform(item_data_scaled)\n",
    "\n",
    "# Display shape of PC_items\n",
    "PC_items.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for convenience, let's put PC_items into a new dataframe.\n",
    "\n",
    "We'll also name the columns and update its index to be the same as the orginal item_data's index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put PC_items into a dataframe\n",
    "items_pca = pd.DataFrame(PC_items)\n",
    "\n",
    "# Name the columns\n",
    "items_pca.columns = ['PC{}'.format(i + 1) for i in range(PC_items.shape[1])]\n",
    "\n",
    "# Update its index\n",
    "items_pca.index = item_data.index\n",
    "\n",
    "# Display first 5 rows\n",
    "items_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we have a dataframe of 300 customer-level principal component features.\n",
    "* These were generated from the 300 principal components that explained the most variance for the original features.\n",
    "* The index of this PCA item dataframe contains CustomerID's, which will make it possible for us to join this to our analytical base table.\n",
    "\n",
    "Finally, save this item dataframe with PCA features as 'pca_item_data.csv'.\n",
    "* Next, we'll compare the clusters made from using these features against those in 'threshold_item_data.csv'.\n",
    "* Do not set index=None because we want to keep the CustomerID's as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pca_item_data.csv\n",
    "items_pca.to_csv('Files/pca_item_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. KMeans Clustering\n",
    "Let's import 3 CSV files we've saved throughout this project.\n",
    "* Let's import 'analytical_base_table.csv' as base_df.\n",
    "* Let's import 'threshold_item_data.csv' as threshold_item_data.\n",
    "* Let's import 'pca_item_data.csv' as pca_item_data.\n",
    "* Set index_col=0 for each one to use CustomerID as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import analytical base table\n",
    "base_df = pd.read_csv('Files/analytical_base_table.csv', index_col=0)\n",
    "\n",
    "# Import thresholded item features\n",
    "threshold_item_data = pd.read_csv('Files/threshold_item_data.csv', index_col=0)\n",
    "\n",
    "# Import PCA item features\n",
    "pca_item_data = pd.read_csv('Files/pca_item_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape of each one to make sure we're on the same page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of each dataframe\n",
    "print( base_df.shape )\n",
    "print( threshold_item_data.shape )\n",
    "print( pca_item_data.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because K-Means creates clusters based on distances, and because distances are calculated by between observations defined by their feature values, **the features you choose to input into the algorithm heavily influence the clusters that are created.**\n",
    "\n",
    "For this project, we will look at 3 possible feature sets and compare the clusters created from them. We'll try:\n",
    "1. Only purchase pattern features (\"Base DF\")\n",
    "2. Purchase pattern features + item features chosen by thresholding (\"Threshold DF\")\n",
    "3. Purchase pattern features + principal component features from items (\"PCA DF\")\n",
    "\n",
    "Create a threshold_df by joining base_df with threshold_item_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join base_df with threshold_item_data\n",
    "threshold_df = base_df.join(threshold_item_data)\n",
    "\n",
    "# Display first 5 rows of threshold_df\n",
    "threshold_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pca_df by joining base_df with pca_item_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join base_df with pca_item_data\n",
    "pca_df = base_df.join(pca_item_data)\n",
    "\n",
    "# Display first 5 rows of pca_df\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of clusters\n",
    "So, how many clusters should you set?\n",
    "* As with much of Unsupervised Learning, there's no right or wrong answer.\n",
    "* Typically, you should consider how your client/key stakeholder will use the clusters.\n",
    "* For example, let's say our client, the online gift retailer, employs 3 customer service reps, and they want to assign one cluster to each rep.\n",
    "* In that case, the obvious answer is 3.\n",
    "* For this project, we'll set the number of clusters to 3. However, you should always feel free to adapt this number depending on what you need.\n",
    "\n",
    "First scale both the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize instance of StandardScaler\n",
    "t_scaler = StandardScaler()\n",
    "p_scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform\n",
    "threshold_df_scaled = t_scaler.fit_transform(threshold_df)\n",
    "pca_df_scaled = p_scaler.fit_transform(pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means with threshold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_kmeans.fit(threshold_df_scaled)\n",
    "threshold_df['cluster'] = t_kmeans.fit_predict(threshold_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot, colored by cluster\n",
    "sns.lmplot(x='total_sales', y='avg_cart_value', hue='cluster', data=threshold_df, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means with pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_kmeans.fit(pca_df_scaled)\n",
    "pca_df['cluster'] = p_kmeans.fit_predict(pca_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot, colored by cluster\n",
    "sns.lmplot(x='total_sales', y='avg_cart_value', hue='cluster', data=pca_df, fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similary between base_df.cluster and threshold_df.cluster\n",
    "adjusted_rand_score(pca_df.cluster, threshold_df.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
